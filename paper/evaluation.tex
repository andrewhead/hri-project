\section{Experiment}

I recruited 28 participants on Mechanical Turk.
We accepted workers for whom 95\% of past tasks were approved.
The design was between-subjects design.
Each participant was assigned either the Nelder-Mead interface or the Bayesian optimization interface, and to one of two goal configurations.
They were shown an image of an engraved `O' produced with the ``goal'' configuration (see Figure~\ref{fig:interfaces}).
They were asked to rank the images based on their closeness to the goal.

They completed the activity when they had either submitted 20 rankings, or had marked the goal configuration as the best.
I chose 20 rankings for three reasons.
First, it appeared reasonable from personal testing that participants could achieve an image close to the provided goals with the initial points chosen with that many rankings, with both interfaces.
Second, the Nelder-Mead method seemed to converge to a set of very similar examples before reaching the 20th iteration.
I wanted to avoid participant attrition when they felt like the algorithm was no longer responding to their rankings.
Third, I needed to impose a limit on the number of rankings a participant submitted to let them finish the task an claim their compensation in a reasonable amount of time.

Participants were not allowed to end their participation by just reporting that they had reached the goal configuration---
the goal configuration actually had to be presented to them by the algorithm, and they had to report it as the best one.
This was to prevent participants from falsely reporting having achieved the goal in order to claim early compensation.
Each participants were paid \$1.00 for this HIT\@.

In the questionnaire, participants were rated their agreement with six statements on a 5-point Likert scale:
\begin{itemize}[noitemsep]
\item I trusted that the algorithm would show me better images in each round.
\item I don't understand why the algorithm chose the examples it did.
\item The algorithm got stuck.  It kept picking images that weren't what I wanted.
\item The last image I marked as best was very similar to the goal image.
\item The algorithm's guesses got better over time.
\item The algorithm made random choices about what to show me.
\end{itemize}
These statements were chosen to capture participants' feelings of the algorithms randomness, trustworthiness and success in attaining a goal, and change in behavior over time.
They were based on Hoffman's recommended metrics for evaluating robot fluency~\cite{hoffman_evaluating_2013}.
Although I was not evaluating fluency, I was interested in assessing perceptions of an algorithm's contribution, trust, and satisfaction, for which Hoffman provided initial ideas.
The order of the questions was not randomized, but it should be in future versions of this study.
