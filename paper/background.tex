\section{Background}

\subsection{Psychology of Optimization}

In this study, I evaluate two methods for eliciting user preference.
Both of these take input as the result of a \emph{comparison} between two or more examples.
I request comparisons from users rather than ratings because past research has shown that people are much more reliable at producing the former.
For example,

While I have not done sufficient work on decision-making capabilities,~\cite{brochu_tutorial_2010} provide a brief but convincing introduction on this.
I paraphrase this discussion here.
The human perceptual apparatus is attuned to evaluate differences rather than absolute magnitudes (according to Tversky \& Kahneman~\cite{kahneman_prospect_1977,tversky_advances_1992}).
While I could not find the find the sources Brochu et al.\ quoted, human evaluation has been shown to be subject to phenomena such as \emph{drift}, where the scale varies over time, and \emph{anchoring}, in which early experiences dominate the scale.
Human beings \emph{do} excel at comparing options and expressing a preference for one over others~\cite{Kingsley_preference_2006}.
Also, Brochu et al.\ references author ``Kendall'', stating that by presenting two or more realizations to a user and requiring only that they indicate preference, we can get far more robust results with much less cognitive burden on the user.

\subsection{Nelder-Mead Algorithm}

Nelder-Mead is a numerical optimization method.~\andrew{Cross-check terminology.}
It is also known as the ``simplex'' method.

It positions points at initial locations.
It then queries a function for evaluations at each of those four locations.
It computes the current centroid of the simplex.
Next, it moves the worst point, or all but the best points, toward the best point by reflecting, expanding, contracting, or reducing it relative to the centroid or the best point.
By opportunistically the simplex in the direction of the best point, the Nelder-Mead moves toward some local maximum.
Depending on the size of the initial simplex and the positioning of the initial points, a simplex in some cases may be able to find the global maximum.

The Nelder-Mead method has been shown to work well, even for non-convex models~\andrew{needs citations}.
We kept typical weights: a reflection coefficient of 1, an expansion coefficient of 2, a contraction coefficient of -1/2, and a reduction coefficient of 1/2.
\andrew{See if we can find these weights in the literature on Nelder-Mead.}

This formulation of Nelder-Mead~\andrew{Need a concrete, non-Wikipedia reference} only needs to know the best, second-to-worst, and worst points out of each set of vertices.
Because of this, the simplex can move towards some maximum as long as a human can rank examples relative to each other.
No objective or numerical assessment of the quality of each vertex is needed.

\subsection{Bayesian Optimization}

Bayesian optimization consists of two steps.

First, it continually learns a Gaussian process.
With this Gaussian process, it predicts the values of a new points.
These points are predicted based on some kernel---
usually the nearest points in an input space are most critical in determining the prediction for a new point.
Based on the observations it has collected so far, Bayesian Optimization produces a Gaussian process that will predict the output for new data points.

Second, it predicts which values should be sampled next.
Typically, this is done with some metric of \emph{expected improvement}.
In some cases, this can be expressed as a linear combination of a term that maximizes exploitation and a term that maximized exploration:
\begin{equation}
EI (x) = (\mu(x) - f (x^+)) \Phi(Z) + \sigma(x) \phi(Z)
\end{equation}
where $Z = \frac{\mu(x) - f(x^+)}{\sigma(x)}$, a term that describes, when used with a normal distribution, the probability of improvement at input $x$.
$\mu(x)$ is the predicted value of a new input $x$.
$f(x^+)$ is highest value seen so far.
$\sigma(x)$ is the variance at $x$.~\andrew{is this the variance or the standard deviation?}
$\phi(Z)$ and $\Phi(Z)$ are the probability and cumulative density functions for $Z$.

Brochu et al.~\cite{brochu_tutorial_2010}~\cite{brochu_active_2008} propose a variant on Bayesian optimization that derives the underlying model based on comparisons alone.
The details are irrelevant for the purposes of this paper.
The algorithm I used is identical to that proposed in Brochu et al.'s tutorial~\cite{brochu_tutorial_2010}.
I varied parameters and the kernel matrix based on what appeared to work best for convergence to a global maximum for my application of finding the right engraving settings.
I used a squared exponential kernel, with $\sigma = 0.25$ and $\sigma_{noise} = 10$.
\andrew{Understand what these mean and whether it was ridiculous to use these parameters.}
