\section{Algorithms}

I implement two algorithms that enable the discovery of user preferences.
Here, I provide a brief introduction to each of the algorithms.
I describe the steps taken to modify the algorithms to accept comparisons as input to drive optimization.
I also describe how bounds are enforced for each one, the initial points of each algorithm, and the hyperparameters used.

\subsection{Nelder-Mead Optimization}

Nelder-Mead is an optimization method that iteratively moves vertices of a simplex in the direction of towards the best-rated vertex~\cite{nelder_simplex_1965}.
For reflection, expansion, and contraction, we chose coefficients observed as the best in the original Nelder-Mead work~\cite{nelder_simplex_1965}:
a reflection coefficient of 1,
an expansion coefficient of 2,
a contraction coefficient of -1/2,
and a reduction coefficient of 1/2.

To perform Nelder-Mead optimization, the algorithm doesn't need to know an exact evaluation for each vertex.
It only needs to know the best, second-to-worst, and worst points out of each set of vertices.

\subsection{Bayesian Optimization}

Each iteration of Bayesian optimization comprises two steps~\cite{brochu_tutorial_2010}:
fitting a Gaussian process to the data seen so far, and sampling a new value likely to maximize the unknown cost function.
Typically, selection of this sample in the second step is performed by maximizing \emph{expected improvement}.
Based on the work by Brochu et al.~\cite{brochu_tutorial_2010}, I express this as combination of two terms for exploration and exploitation:
\begin{equation}
EI (x) = (\mu(x) - f (x^+)) \Phi(Z) + \sigma(x) \phi(Z)
\end{equation}
where $Z = \frac{\mu(x) - f(x^+)}{\sigma(x)}$ is in the domain of a normal distribution, representing the probability of improvement at $x$;
$\mu(x)$ is the predicted value of a new input $x$;
$f(x^+)$ is highest value seen so far;
$\sigma(x)$ is the standard deviation at $x$.
$\phi(Z)$ and $\Phi(Z)$ are the probability and cumulative density functions for $Z$.

\if 0
With this Gaussian process, it predicts the values of a new points.
These points are predicted based on some kernel---
usually the nearest points in an input space are most critical in determining the prediction for a new point.
Based on the observations it has collected so far, Bayesian Optimization produces a Gaussian process that will predict the output for new data points.
\fi

Brochu et al.~\cite{brochu_tutorial_2010}~\cite{brochu_active_2008} propose a variant on Bayesian optimization that takes comparisons between sampled points as inputs.
I implement this algorithm for this study.
I used a squared exponential kernel with $\sigma = 0.25$.
In performing Newton-Rhapson optimization according to Brochu et al.'s formulation, $\sigma_{noise}$ was set to 10.
These parameters were chosen based on what appeared to enable convergence in the target 3D input space for this study.
