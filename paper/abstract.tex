\begin{abstract}
People frequently work with machines to produce artifacts.
While they know their end goal, it can be difficult to express it with the parameters the machine makes available.
This paper studies algorithms to help users explore a space of unfamiliar machine parameters in pursuit of a known goal.
It aims to understand the performance of two algorithms in terms of speed of convergence to the human's goal, and the human's trust.
The algorithms are Nelder-Mead and a variant of Bayesian optimization.
I build for front-end for each one where a user provides comparison-based ratings of examples to guide the algorithm.
With a 28-participant Mechanical Turk study, I compare the algorithms.
While a Bayesian Optimization algorithm appeared more random to participants, they felt that it better achieved the goal.
Nelder-Mead appeared to get better over time to participants, it seemed more likely to get stuck.
The study provides justification and criteria for a systematic evaluation of the affordances, human perceptions, and performance of algorithms for exploring parameter spaces.
\end{abstract}
