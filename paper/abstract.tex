\begin{abstract}
People frequently work with machines to produce artifacts.
While they know their end goal, it can be difficult to express it with the parameters the machine makes available.
This paper studies algorithms to help users explore a space of unfamiliar machine parameters in pursuit of a known goal.
It aims to understand the performance of two algorithms in terms of speed of convergence to the human's goal, and the human perceptions of randomness, convergence, and success.
The algorithms are Nelder-Mead and a variant of Bayesian optimization.
I build for front-end for each one where a user provides comparison-based ratings to guide the algorithm.
With a 28-participant Mechanical Turk study, I compare the algorithms.
While a Bayesian Optimization algorithm appeared more random to participants, they felt that it better achieved the goal.
Nelder-Mead appeared to get better over time to participants, but also looks like it gets stuck.
The study justifies a systematic evaluation of the affordances, human perceptions, and performance of algorithms for exploring parameter spaces.
\end{abstract}
